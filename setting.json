{
    "path": {
        "data": "data",
        "code": "code",
        "log": "log",
        "model": "model",
        "state_dict": "state_dict",
        "submit": "submit",
        "train": "train",
        "valid": "valid",
        "ensemble": "ensemble",
        "best_model": "best_model"
    },

    "file_name": {
        "train": "2023-05-22_train_data",
        "test": "2023-05-22_test_data"
    },

    "save_name": "",
    "num_kfolds": 1,

    "seed": 42,
    "device": "cuda",

    "fixed_train_columns": [
        "user_id",
        "answer_code"
    ],

    "fixed_embedding_columns": ["question_id"],

    "selective_train_columns": [
        "knowledge_tag",
        "user_correct_answer",
        "user_total_answer",
        "user_acc",
        "test_mean",
        "test_sum",
        "tag_mean",
        "tag_sum"
    ],

    "selective_embedding_columns": ["test_id", "knowledge_tag"],

    "predict_column": "answer_code",

    "train_valid_split": 0.9,
    "max_train_length": 100,
    "extra_split": false,




    "model_name": "lgbm",
    "loss_fn": "BCEWLL",
    "optimizer": "ADAM",
    "scheduler": "plateau",
    "epoch": 1,
    "batch_size": 16,
    "num_workers": 1,
    "wandb_activate" : true,
    "patience" : 10,
    "best_model_activate": true,

    "adam": {
        "learn_rate": 0.01,
        "weight_decay": 0
    },

    "sgd": {
        "learn_rate": 0.0001,
        "weight_decay": 0.01
    },

    "rmsprop": {
        "learn_rate": 0.0001,
        "weight_decay": 0.01
    },

    "adadelta": {
        "learn_rate": 0.0001,
        "weight_decay": 0.01
    },

    "plateau": {
        "patience": 10,
        "factor": 0.5,
        "mode": "max",
        "verbose": true
    },

    "lstm": {
        "embedding_dim": 21,
        "input_dim": 64,
        "output_dim": 64,
        "max_seq_len": 20,
        "n_layers": 2,
        "dense_layer_dim": [16, 4]
    },

    "lstm_attn": {
        "embedding_dim": 21,
        "input_dim": 64,
        "output_dim": 64,
        "max_seq_len": 20,
        "n_layers": 2,
        "n_heads": 2,
        "drop_out": 0.2,
        "dense_layer_dim": [16, 4]
    },

    "bert": {
        "embedding_dim": 21,
        "input_dim": 64,
        "max_seq_len": 20,
        "n_layers": 2,
        "n_heads": 2,
        "dense_layer_dim": [16, 4]
    },

    "lgcn": {
        "embedding_dim": 64,
        "num_layers": 1,
        "alpha": 0
    },

    "lgbm": {
        "params":{
            "objective" : "binary",
            "learning_rate": 0.01
        },
        "verbose_eval": 100,
        "num_boost_round":2000,
        "early_stopping_rounds":450
    }
}
